{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install snntorch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T11:16:40.202915Z","iopub.execute_input":"2025-07-17T11:16:40.203472Z","iopub.status.idle":"2025-07-17T11:16:45.107264Z","shell.execute_reply.started":"2025-07-17T11:16:40.203435Z","shell.execute_reply":"2025-07-17T11:16:45.106564Z"}},"outputs":[{"name":"stdout","text":"Collecting snntorch\n  Downloading snntorch-0.9.4-py2.py3-none-any.whl.metadata (15 kB)\nDownloading snntorch-0.9.4-py2.py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: snntorch\nSuccessfully installed snntorch-0.9.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport snntorch as snn\nfrom snntorch import surrogate\nfrom snntorch import backprop\nfrom snntorch import functional as SF\nfrom snntorch import utils\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport re\nfrom collections import Counter\nimport math\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\nbatch_size = 16  # Reduced for better learning\nnum_epochs = 10\nlearning_rate = 1e-4  # More conservative learning rate\nnum_steps = 50  # Reduced time steps for efficiency\nvocab_size = 30522  # BERT vocab size\nembedding_dim = 256\nhidden_dim = 512\nnum_classes = 2\nbeta_stm = 0.8  # STM decay (fast)\nbeta_ltm = 0.95  # LTM decay (slow)\nthreshold = 1.0\nmax_length = 128  # Reduced for memory efficiency\ndropout_rate = 0.4  # Increased dropout for better regularization\n\nclass IMDBDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        # Tokenize using pretrained tokenizer\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            truncation=True,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        return encoding['input_ids'].squeeze(), torch.tensor(label, dtype=torch.long)\n\nclass AdaptiveSTDPLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, beta, threshold=1.0):\n        super(AdaptiveSTDPLayer, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.beta = beta\n        self.threshold = threshold\n        \n        # Learnable parameters with proper initialization\n        self.linear = nn.Linear(input_dim, output_dim)\n        self.layer_norm = nn.LayerNorm(output_dim)\n        \n        # Initialize weights properly\n        nn.init.kaiming_uniform_(self.linear.weight, a=math.sqrt(5))\n        nn.init.constant_(self.linear.bias, 0)\n        \n        # Use ATan surrogate for better gradient flow\n        self.spike_grad = surrogate.atan(alpha=2.0)\n        self.lif = snn.Leaky(beta=beta, threshold=threshold, spike_grad=self.spike_grad)\n        \n        # Adaptive dropout\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Improved STDP parameters\n        self.register_buffer('tau_plus', torch.tensor(15.0))\n        self.register_buffer('tau_minus', torch.tensor(30.0))\n        self.A_plus = 0.001\n        self.A_minus = 0.001\n        \n        # Trace variables for STDP\n        self.register_buffer('pre_trace', torch.zeros(1, input_dim))\n        self.register_buffer('post_trace', torch.zeros(1, output_dim))\n        \n    def forward(self, x, mem=None):\n        batch_size = x.size(0)\n        \n        # Initialize membrane potential if needed\n        if mem is None:\n            mem = self.lif.init_leaky()\n        \n        # Apply layer normalization before linear transformation\n        x_norm = self.layer_norm(x) if x.dim() > 1 else x\n        \n        # Apply dropout during training\n        if self.training:\n            x_norm = self.dropout(x_norm)\n        \n        # Linear transformation\n        cur = self.linear(x_norm)\n        \n        # Add noise during training for regularization\n        if self.training:\n            noise = torch.randn_like(cur) * 0.01\n            cur = cur + noise\n        \n        # LIF neuron processing\n        spk, mem = self.lif(cur, mem)\n        \n        # Apply STDP learning during training\n        if self.training:\n            self.apply_stdp(x_norm, spk)\n        \n        return spk, mem\n    \n    def apply_stdp(self, pre_spikes, post_spikes):\n        \"\"\"Apply STDP learning rule\"\"\"\n        if not self.training:\n            return\n        \n        batch_size = pre_spikes.size(0)\n        \n        # Create new tensors for traces by cloning after expansion\n        pre_trace = self.pre_trace.expand(batch_size, -1).clone()\n        post_trace = self.post_trace.expand(batch_size, -1).clone()\n        \n        # Decay traces\n        pre_trace = pre_trace * torch.exp(-1.0 / self.tau_plus)\n        post_trace = post_trace * torch.exp(-1.0 / self.tau_minus)\n        \n        # Update traces with current spikes\n        pre_trace = pre_trace + pre_spikes\n        post_trace = post_trace + post_spikes\n        \n        # Update the buffers with the new values\n        self.pre_trace = pre_trace.mean(dim=0, keepdim=True)\n        self.post_trace = post_trace.mean(dim=0, keepdim=True)\n        \n        # Compute STDP weight updates\n        with torch.no_grad():\n            # LTP: post-synaptic spike increases weights\n            ltp_update = torch.outer(post_spikes.mean(0), pre_trace.mean(0))\n            \n            # LTD: pre-synaptic spike decreases weights\n            ltd_update = torch.outer(post_trace.mean(0), pre_spikes.mean(0))\n            \n            # Apply updates with small learning rate\n            weight_update = self.A_plus * ltp_update - self.A_minus * ltd_update\n            \n            # Apply weight update with bounds\n            self.linear.weight.data += weight_update * 0.0001\n            self.linear.weight.data = torch.clamp(self.linear.weight.data, -2.0, 2.0)\n\nclass MemoryGatingLayer(nn.Module):\n    \"\"\"Gating mechanism to control STM/LTM contribution\"\"\"\n    def __init__(self, hidden_dim):\n        super(MemoryGatingLayer, self).__init__()\n        self.gate_linear = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.gate_activation = nn.Sigmoid()\n        \n    def forward(self, stm_output, ltm_output):\n        # Concatenate STM and LTM outputs\n        combined = torch.cat([stm_output, ltm_output], dim=-1)\n        \n        # Compute gate values\n        gate = self.gate_activation(self.gate_linear(combined))\n        \n        # Apply gating\n        gated_output = gate * stm_output + (1 - gate) * ltm_output\n        \n        return gated_output\n\nclass NeuromorphicSentimentNet(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, \n                 beta_stm, beta_ltm, threshold):\n        super(NeuromorphicSentimentNet, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_steps = num_steps\n        \n        # Embedding layer with proper initialization\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        nn.init.normal_(self.embedding.weight, 0, 0.1)\n        \n        # Rate encoding with batch normalization\n        self.rate_encoder = nn.Sequential(\n            nn.Linear(embedding_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Tanh()\n        )\n        \n        # STM Layer (fast dynamics) - processes immediate context\n        self.stm_layer = AdaptiveSTDPLayer(hidden_dim, hidden_dim, beta_stm, threshold)\n        \n        # LTM Layer (slow dynamics) - processes long-term patterns\n        self.ltm_layer = AdaptiveSTDPLayer(hidden_dim, hidden_dim, beta_ltm, threshold)\n        \n        # Memory gating mechanism\n        self.memory_gate = MemoryGatingLayer(hidden_dim)\n        \n        # Attention mechanism for sequence processing\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim,\n            num_heads=8,\n            dropout=dropout_rate,\n            batch_first=True\n        )\n        \n        # Final integration layers\n        self.integration_norm = nn.LayerNorm(hidden_dim)\n        self.integration_dropout = nn.Dropout(dropout_rate)\n        \n        # Classification head with regularization\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_dim // 2, num_classes)\n        )\n        \n        # Initialize classifier weights\n        for layer in self.classifier:\n            if isinstance(layer, nn.Linear):\n                nn.init.kaiming_uniform_(layer.weight)\n                nn.init.constant_(layer.bias, 0)\n        \n    def forward(self, x):\n        batch_size, seq_len = x.shape\n        \n        # Initialize memory states\n        stm_mem = self.stm_layer.lif.init_leaky()\n        ltm_mem = self.ltm_layer.lif.init_leaky()\n        \n        # Store outputs over time\n        stm_outputs = []\n        ltm_outputs = []\n        \n        # Process sequence with time steps\n        effective_seq_len = min(seq_len, self.num_steps)\n        \n        for t in range(effective_seq_len):\n            # Get embeddings for current time step\n            emb = self.embedding(x[:, t])\n            \n            # Rate encoding - convert to neural activity\n            rate_input = self.rate_encoder(emb)\n            \n            # Generate spikes with proper normalization\n            spike_input = torch.sigmoid(rate_input) * 2.0 - 1.0\n            \n            # STM processing (fast adaptation)\n            stm_spk, stm_mem = self.stm_layer(spike_input, stm_mem)\n            stm_outputs.append(stm_spk)\n            \n            # LTM processing (slow adaptation)\n            ltm_spk, ltm_mem = self.ltm_layer(spike_input, ltm_mem)\n            ltm_outputs.append(ltm_spk)\n        \n        # Aggregate outputs over time\n        if stm_outputs and ltm_outputs:\n            # Stack and process temporal information\n            stm_sequence = torch.stack(stm_outputs, dim=1)  # [batch, time, hidden]\n            ltm_sequence = torch.stack(ltm_outputs, dim=1)  # [batch, time, hidden]\n            \n            # Apply attention to capture important temporal patterns\n            stm_attended, _ = self.attention(stm_sequence, stm_sequence, stm_sequence)\n            ltm_attended, _ = self.attention(ltm_sequence, ltm_sequence, ltm_sequence)\n            \n            # Temporal pooling with weighted average\n            time_weights = torch.softmax(torch.arange(effective_seq_len, dtype=torch.float32, device=x.device), dim=0)\n            stm_pooled = (stm_attended * time_weights.view(1, -1, 1)).sum(dim=1)\n            ltm_pooled = (ltm_attended * time_weights.view(1, -1, 1)).sum(dim=1)\n            \n        else:\n            stm_pooled = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n            ltm_pooled = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n        \n        # Apply memory gating\n        gated_output = self.memory_gate(stm_pooled, ltm_pooled)\n        \n        # Final normalization and dropout\n        integrated = self.integration_norm(gated_output)\n        integrated = self.integration_dropout(integrated)\n        \n        # Classification\n        output = self.classifier(integrated)\n        \n        return output\n\ndef load_imdb_data():\n    \"\"\"Load real IMDB dataset from Hugging Face\"\"\"\n    print(\"Loading IMDB dataset...\")\n    \n    # Load dataset\n    dataset = load_dataset('imdb')\n    \n    # Extract train and test data\n    train_texts = dataset['train']['text']\n    train_labels = dataset['train']['label']\n    test_texts = dataset['test']['text']\n    test_labels = dataset['test']['label']\n    \n    # Use a reasonable subset for development\n    # train_texts = train_texts[:20000]  # Smaller subset for better learning\n    # train_labels = train_labels[:20000]\n    # test_texts = test_texts[:10000]\n    # test_labels = test_labels[:10000]\n    \n    print(f\"Training samples: {len(train_texts)}\")\n    print(f\"Test samples: {len(test_texts)}\")\n    \n    return train_texts, train_labels, test_texts, test_labels\n\ndef validate_model(model, val_loader, criterion):\n    \"\"\"Validate the model\"\"\"\n    model.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        for data, targets in val_loader:\n            data, targets = data.to(device), targets.to(device)\n            outputs = model(data)\n            loss = criterion(outputs, targets)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            val_total += targets.size(0)\n            val_correct += predicted.eq(targets).sum().item()\n    \n    val_accuracy = 100. * val_correct / val_total\n    avg_val_loss = val_loss / len(val_loader)\n    \n    return avg_val_loss, val_accuracy\n\ndef train_model():\n    # Load real IMDB data\n    train_texts, train_labels, test_texts, test_labels = load_imdb_data()\n    \n    # Initialize pretrained tokenizer\n    print(\"Loading pretrained tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    \n    # Add padding token if not present\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Create train/validation split\n    train_texts_split, val_texts, train_labels_split, val_labels = train_test_split(\n        train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n    )\n    \n    # Create datasets\n    train_dataset = IMDBDataset(train_texts_split, train_labels_split, tokenizer, max_length=max_length)\n    val_dataset = IMDBDataset(val_texts, val_labels, tokenizer, max_length=max_length)\n    test_dataset = IMDBDataset(test_texts, test_labels, tokenizer, max_length=max_length)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Initialize model\n    model = NeuromorphicSentimentNet(\n        vocab_size=tokenizer.vocab_size,\n        embedding_dim=embedding_dim,\n        hidden_dim=hidden_dim,\n        num_classes=num_classes,\n        beta_stm=beta_stm,\n        beta_ltm=beta_ltm,\n        threshold=threshold\n    ).to(device)\n    \n    # Loss and optimizer with proper regularization\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing helps generalization\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-3)\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n    \n    # Early stopping\n    best_val_accuracy = 0\n    patience = 3\n    patience_counter = 0\n    \n    # Training loop\n    print(\"Starting training...\")\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (data, targets) in enumerate(train_loader):\n            data, targets = data.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(data)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass\n            loss.backward()\n            \n            # Gradient clipping for stability\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n            \n            optimizer.step()\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            if batch_idx % 50 == 0:\n                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n        \n        # Update learning rate\n        scheduler.step()\n        \n        # Training metrics\n        train_accuracy = 100. * correct / total\n        avg_train_loss = total_loss / len(train_loader)\n        \n        # Validation\n        val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n        \n        print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n        \n        # Early stopping check\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            patience_counter = 0\n            # Save best model\n            torch.save(model.state_dict(), 'best_neuromorphic_sentiment_model.pth')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch}\")\n                break\n    \n    # Load best model for final evaluation\n    model.load_state_dict(torch.load('best_neuromorphic_sentiment_model.pth'))\n    \n    # Final test evaluation\n    test_loss, test_accuracy = validate_model(model, test_loader, criterion)\n    print(f'Final Test Accuracy: {test_accuracy:.2f}%')\n    \n    return model, tokenizer\n\ndef predict_sentiment(model, tokenizer, text):\n    \"\"\"Predict sentiment of a single text\"\"\"\n    model.eval()\n    with torch.no_grad():\n        encoding = tokenizer(\n            text,\n            max_length=max_length,\n            truncation=True,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        tokens = encoding['input_ids'].to(device)\n        output = model(tokens)\n        prediction = torch.softmax(output, dim=1)\n        sentiment = \"Positive\" if prediction[0][1] > prediction[0][0] else \"Negative\"\n        confidence = prediction[0].max().item()\n        return sentiment, confidence\n\nif __name__ == \"__main__\":\n    print(\"Training Enhanced Neuromorphic Sentiment Analysis Model...\")\n    \n    model, tokenizer = train_model()\n    \n    # Test with sample texts\n    test_texts = [\n        \"This movie is absolutely amazing! The cinematography was breathtaking and the acting was superb.\",\n        \"I hate this film, it's terrible. The plot made no sense and the dialogue was awful.\",\n        \"Not bad, but could be better. The story was okay but felt rushed.\",\n        \"Outstanding performance by all actors! This is definitely one of the best films I've seen this year.\",\n        \"The movie was boring and predictable. I fell asleep halfway through.\",\n        \"Incredible storytelling and beautiful visuals. A masterpiece of modern cinema!\"\n    ]\n    \n    print(\"\\nTesting predictions:\")\n    for text in test_texts:\n        sentiment, confidence = predict_sentiment(model, tokenizer, text)\n        print(f\"Text: '{text[:50]}...' -> {sentiment} (confidence: {confidence:.3f})\")\n    \n    print(\"\\nModel training completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:24:11.512458Z","iopub.execute_input":"2025-07-16T15:24:11.512758Z","iopub.status.idle":"2025-07-16T16:02:06.463605Z","shell.execute_reply.started":"2025-07-16T15:24:11.512733Z","shell.execute_reply":"2025-07-16T16:02:06.462906Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/4104190816.py:6: DeprecationWarning: The module snntorch.backprop will be deprecated in  a future release. Writing out your own training loop will lead to substantially faster performance.\n  from snntorch import backprop\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nTraining Enhanced Neuromorphic Sentiment Analysis Model...\nLoading IMDB dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca4cd2bd2ad246a59525b5568963e72d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d688cbd710124b64b5f31615d9c01aec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b170cd2a9884abdab392d18312e5733"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e9eddffb0a94f30af3e0e50ee2bfd6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a14c3c8f2604b11bb7af90f98512a19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd10dc8b68324351b8f028af54ab1ca8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dc8903c4216447189216ce2cd978898"}},"metadata":{}},{"name":"stdout","text":"Training samples: 25000\nTest samples: 25000\nLoading pretrained tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20cfe2f12e504b41bdbd0ef59b08d524"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c787a187ce416b9f396bc0b88b45fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec5236bb9524f3193aec94102e7a633"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f421c2dcd3491eb5f63c774579528d"}},"metadata":{}},{"name":"stdout","text":"Starting training...\nEpoch: 0, Batch: 0, Loss: 1.2335\nEpoch: 0, Batch: 50, Loss: 0.8168\nEpoch: 0, Batch: 100, Loss: 0.7877\nEpoch: 0, Batch: 150, Loss: 0.7026\nEpoch: 0, Batch: 200, Loss: 0.7388\nEpoch: 0, Batch: 250, Loss: 0.6252\nEpoch: 0, Batch: 300, Loss: 0.6522\nEpoch: 0, Batch: 350, Loss: 0.8687\nEpoch: 0, Batch: 400, Loss: 0.6657\nEpoch: 0, Batch: 450, Loss: 0.7649\nEpoch: 0, Batch: 500, Loss: 0.6524\nEpoch: 0, Batch: 550, Loss: 0.5995\nEpoch: 0, Batch: 600, Loss: 0.6311\nEpoch: 0, Batch: 650, Loss: 0.7365\nEpoch: 0, Batch: 700, Loss: 0.6433\nEpoch: 0, Batch: 750, Loss: 0.6755\nEpoch: 0, Batch: 800, Loss: 0.7112\nEpoch: 0, Batch: 850, Loss: 0.6084\nEpoch: 0, Batch: 900, Loss: 0.5392\nEpoch: 0, Batch: 950, Loss: 0.6591\nEpoch: 0, Batch: 1000, Loss: 0.4868\nEpoch: 0, Batch: 1050, Loss: 0.7371\nEpoch: 0, Batch: 1100, Loss: 0.6663\nEpoch: 0, Batch: 1150, Loss: 0.7411\nEpoch: 0, Batch: 1200, Loss: 0.5828\nEpoch 0: Train Loss: 0.7017, Train Acc: 59.44%, Val Loss: 0.5926, Val Acc: 71.12%\nEpoch: 1, Batch: 0, Loss: 0.5943\nEpoch: 1, Batch: 50, Loss: 0.4490\nEpoch: 1, Batch: 100, Loss: 0.4360\nEpoch: 1, Batch: 150, Loss: 0.5100\nEpoch: 1, Batch: 200, Loss: 0.6447\nEpoch: 1, Batch: 250, Loss: 0.7259\nEpoch: 1, Batch: 300, Loss: 0.5794\nEpoch: 1, Batch: 350, Loss: 0.3954\nEpoch: 1, Batch: 400, Loss: 0.7617\nEpoch: 1, Batch: 450, Loss: 0.5033\nEpoch: 1, Batch: 500, Loss: 0.4029\nEpoch: 1, Batch: 550, Loss: 0.4655\nEpoch: 1, Batch: 600, Loss: 0.5491\nEpoch: 1, Batch: 650, Loss: 0.5086\nEpoch: 1, Batch: 700, Loss: 0.6441\nEpoch: 1, Batch: 750, Loss: 0.4847\nEpoch: 1, Batch: 800, Loss: 0.4923\nEpoch: 1, Batch: 850, Loss: 0.4503\nEpoch: 1, Batch: 900, Loss: 0.7414\nEpoch: 1, Batch: 950, Loss: 0.6122\nEpoch: 1, Batch: 1000, Loss: 0.6524\nEpoch: 1, Batch: 1050, Loss: 0.6116\nEpoch: 1, Batch: 1100, Loss: 0.6185\nEpoch: 1, Batch: 1150, Loss: 0.4526\nEpoch: 1, Batch: 1200, Loss: 0.4016\nEpoch 1: Train Loss: 0.5637, Train Acc: 74.11%, Val Loss: 0.5540, Val Acc: 75.14%\nEpoch: 2, Batch: 0, Loss: 0.4698\nEpoch: 2, Batch: 50, Loss: 0.5330\nEpoch: 2, Batch: 100, Loss: 0.5288\nEpoch: 2, Batch: 150, Loss: 0.7644\nEpoch: 2, Batch: 200, Loss: 0.5883\nEpoch: 2, Batch: 250, Loss: 0.5069\nEpoch: 2, Batch: 300, Loss: 0.6079\nEpoch: 2, Batch: 350, Loss: 0.4479\nEpoch: 2, Batch: 400, Loss: 0.4106\nEpoch: 2, Batch: 450, Loss: 0.4707\nEpoch: 2, Batch: 500, Loss: 0.4348\nEpoch: 2, Batch: 550, Loss: 0.9287\nEpoch: 2, Batch: 600, Loss: 0.5530\nEpoch: 2, Batch: 650, Loss: 0.4544\nEpoch: 2, Batch: 700, Loss: 0.6472\nEpoch: 2, Batch: 750, Loss: 0.5328\nEpoch: 2, Batch: 800, Loss: 0.5791\nEpoch: 2, Batch: 850, Loss: 0.5902\nEpoch: 2, Batch: 900, Loss: 0.5207\nEpoch: 2, Batch: 950, Loss: 0.5958\nEpoch: 2, Batch: 1000, Loss: 0.3846\nEpoch: 2, Batch: 1050, Loss: 0.6584\nEpoch: 2, Batch: 1100, Loss: 0.4144\nEpoch: 2, Batch: 1150, Loss: 0.2687\nEpoch: 2, Batch: 1200, Loss: 0.5289\nEpoch 2: Train Loss: 0.4924, Train Acc: 80.26%, Val Loss: 0.5993, Val Acc: 74.58%\nEpoch: 3, Batch: 0, Loss: 0.6138\nEpoch: 3, Batch: 50, Loss: 0.3569\nEpoch: 3, Batch: 100, Loss: 0.4277\nEpoch: 3, Batch: 150, Loss: 0.3408\nEpoch: 3, Batch: 200, Loss: 0.4059\nEpoch: 3, Batch: 250, Loss: 0.5167\nEpoch: 3, Batch: 300, Loss: 0.3568\nEpoch: 3, Batch: 350, Loss: 0.5110\nEpoch: 3, Batch: 400, Loss: 0.4529\nEpoch: 3, Batch: 450, Loss: 0.3929\nEpoch: 3, Batch: 500, Loss: 0.5788\nEpoch: 3, Batch: 550, Loss: 0.4638\nEpoch: 3, Batch: 600, Loss: 0.3404\nEpoch: 3, Batch: 650, Loss: 0.2302\nEpoch: 3, Batch: 700, Loss: 0.3165\nEpoch: 3, Batch: 750, Loss: 0.4085\nEpoch: 3, Batch: 800, Loss: 0.4336\nEpoch: 3, Batch: 850, Loss: 0.3953\nEpoch: 3, Batch: 900, Loss: 0.3418\nEpoch: 3, Batch: 950, Loss: 0.4617\nEpoch: 3, Batch: 1000, Loss: 0.3133\nEpoch: 3, Batch: 1050, Loss: 0.3274\nEpoch: 3, Batch: 1100, Loss: 0.3467\nEpoch: 3, Batch: 1150, Loss: 0.4189\nEpoch: 3, Batch: 1200, Loss: 0.4540\nEpoch 3: Train Loss: 0.4487, Train Acc: 83.89%, Val Loss: 0.5638, Val Acc: 76.28%\nEpoch: 4, Batch: 0, Loss: 0.5653\nEpoch: 4, Batch: 50, Loss: 0.4180\nEpoch: 4, Batch: 100, Loss: 0.4229\nEpoch: 4, Batch: 150, Loss: 0.5331\nEpoch: 4, Batch: 200, Loss: 0.3722\nEpoch: 4, Batch: 250, Loss: 0.3738\nEpoch: 4, Batch: 300, Loss: 0.6780\nEpoch: 4, Batch: 350, Loss: 0.3392\nEpoch: 4, Batch: 400, Loss: 0.3955\nEpoch: 4, Batch: 450, Loss: 0.3164\nEpoch: 4, Batch: 500, Loss: 0.3292\nEpoch: 4, Batch: 550, Loss: 0.4253\nEpoch: 4, Batch: 600, Loss: 0.2849\nEpoch: 4, Batch: 650, Loss: 0.6327\nEpoch: 4, Batch: 700, Loss: 0.6296\nEpoch: 4, Batch: 750, Loss: 0.2641\nEpoch: 4, Batch: 800, Loss: 0.3860\nEpoch: 4, Batch: 850, Loss: 0.3274\nEpoch: 4, Batch: 900, Loss: 0.4545\nEpoch: 4, Batch: 950, Loss: 0.2586\nEpoch: 4, Batch: 1000, Loss: 0.4539\nEpoch: 4, Batch: 1050, Loss: 0.3485\nEpoch: 4, Batch: 1100, Loss: 0.4183\nEpoch: 4, Batch: 1150, Loss: 0.5206\nEpoch: 4, Batch: 1200, Loss: 0.8241\nEpoch 4: Train Loss: 0.4127, Train Acc: 86.78%, Val Loss: 0.5831, Val Acc: 74.86%\nEpoch: 5, Batch: 0, Loss: 0.2311\nEpoch: 5, Batch: 50, Loss: 0.4045\nEpoch: 5, Batch: 100, Loss: 0.5298\nEpoch: 5, Batch: 150, Loss: 0.8743\nEpoch: 5, Batch: 200, Loss: 0.3897\nEpoch: 5, Batch: 250, Loss: 0.3910\nEpoch: 5, Batch: 300, Loss: 0.3535\nEpoch: 5, Batch: 350, Loss: 0.3255\nEpoch: 5, Batch: 400, Loss: 0.4346\nEpoch: 5, Batch: 450, Loss: 0.4351\nEpoch: 5, Batch: 500, Loss: 0.6611\nEpoch: 5, Batch: 550, Loss: 0.3738\nEpoch: 5, Batch: 600, Loss: 0.5380\nEpoch: 5, Batch: 650, Loss: 0.2783\nEpoch: 5, Batch: 700, Loss: 0.4680\nEpoch: 5, Batch: 750, Loss: 0.4946\nEpoch: 5, Batch: 800, Loss: 0.3525\nEpoch: 5, Batch: 850, Loss: 0.2867\nEpoch: 5, Batch: 900, Loss: 0.4129\nEpoch: 5, Batch: 950, Loss: 0.4286\nEpoch: 5, Batch: 1000, Loss: 0.2713\nEpoch: 5, Batch: 1050, Loss: 0.5313\nEpoch: 5, Batch: 1100, Loss: 0.3882\nEpoch: 5, Batch: 1150, Loss: 0.4252\nEpoch: 5, Batch: 1200, Loss: 0.6205\nEpoch 5: Train Loss: 0.3846, Train Acc: 88.97%, Val Loss: 0.5930, Val Acc: 75.68%\nEpoch: 6, Batch: 0, Loss: 0.2212\nEpoch: 6, Batch: 50, Loss: 0.2707\nEpoch: 6, Batch: 100, Loss: 0.3690\nEpoch: 6, Batch: 150, Loss: 0.3660\nEpoch: 6, Batch: 200, Loss: 0.2624\nEpoch: 6, Batch: 250, Loss: 0.3832\nEpoch: 6, Batch: 300, Loss: 0.2527\nEpoch: 6, Batch: 350, Loss: 0.3304\nEpoch: 6, Batch: 400, Loss: 0.4803\nEpoch: 6, Batch: 450, Loss: 0.2215\nEpoch: 6, Batch: 500, Loss: 0.2542\nEpoch: 6, Batch: 550, Loss: 0.3472\nEpoch: 6, Batch: 600, Loss: 0.3668\nEpoch: 6, Batch: 650, Loss: 0.3016\nEpoch: 6, Batch: 700, Loss: 0.3317\nEpoch: 6, Batch: 750, Loss: 0.4397\nEpoch: 6, Batch: 800, Loss: 0.2574\nEpoch: 6, Batch: 850, Loss: 0.2343\nEpoch: 6, Batch: 900, Loss: 0.5044\nEpoch: 6, Batch: 950, Loss: 0.2940\nEpoch: 6, Batch: 1000, Loss: 0.3733\nEpoch: 6, Batch: 1050, Loss: 0.4144\nEpoch: 6, Batch: 1100, Loss: 0.3174\nEpoch: 6, Batch: 1150, Loss: 0.2838\nEpoch: 6, Batch: 1200, Loss: 0.3844\nEpoch 6: Train Loss: 0.3636, Train Acc: 90.33%, Val Loss: 0.6558, Val Acc: 74.88%\nEarly stopping at epoch 6\nFinal Test Accuracy: 72.63%\n\nTesting predictions:\nText: 'This movie is absolutely amazing! The cinematograp...' -> Positive (confidence: 0.944)\nText: 'I hate this film, it's terrible. The plot made no ...' -> Negative (confidence: 0.930)\nText: 'Not bad, but could be better. The story was okay b...' -> Negative (confidence: 0.884)\nText: 'Outstanding performance by all actors! This is def...' -> Positive (confidence: 0.959)\nText: 'The movie was boring and predictable. I fell aslee...' -> Negative (confidence: 0.925)\nText: 'Incredible storytelling and beautiful visuals. A m...' -> Positive (confidence: 0.955)\n\nModel training completed successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}